---
published: true
layout: single
title: SPARK 설치
categories:
  - DE
tags:
  - SPARK

toc: true
toc_sticky: true

---

[설치 및 환경설정](https://mengu.tistory.com/25)
pyspark를 사용하기위해

java,spark,hadoop,pyspark를 설치하고 환경설정을 했다.  
그런데 막상 pyspark를 실행하니 python 경로를 찾을 수 없다는 에러가 떴다.  

이부분을 python 경로 설정을 체크해보니 문제가없었고 구글링을 해보니 java 환경 변수 설정에서 jdk를 그냥 java로 이름을 바꾸고 설정한게원인이었다.  

그래서 원래대로 java폴더 내의 jdk-18.0.2.1폴더에 넣고 설정을 하니 이문제는 해결되었다.  

그런데 새로운 에러와 함께 중간에 멈추게되었다.
<details>
<summary>에러문</summary>
<div markdown="1">


``` python
(base) C:\WINDOWS\system32>pyspark Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)] :: Anaconda, Inc. on win32 Type "help", "copyright", "credits" or "license" for more information. Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties Setting default log level to "WARN". To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel). 22/01/26 01:18:13 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable 22/01/26 01:18:13 WARN SparkContext: Another SparkContext is being constructed (or threw an exception in its constructor). This may indicate an error, since only one SparkContext should be running in this JVM (see SPARK-2243). The other SparkContext was created at: org.apache.spark.api.java.JavaSparkContext.(JavaSparkContext.scala:58) java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77) java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:499) java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:480) py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247) py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357) py4j.Gateway.invoke(Gateway.java:238) py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80) py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69) py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182) py4j.ClientServerConnection.run(ClientServerConnection.java:106) java.base/java.lang.Thread.run(Thread.java:833) C:\Spark\spark-3.2.0-bin-hadoop2.7\python\pyspark\shell.py:42: UserWarning: Failed to initialize Spark session. warnings.warn("Failed to initialize Spark session.") Traceback (most recent call last): File "C:\Spark\spark-3.2.0-bin-hadoop2.7\python\pyspark\shell.py", line 38, in spark = SparkSession._create_shell_session() # type: ignore File "C:\Spark\spark-3.2.0-bin-hadoop2.7\python\pyspark\sql\session.py", line 553, in _create_shell_session return SparkSession.builder.getOrCreate() File "C:\Spark\spark-3.2.0-bin-hadoop2.7\python\pyspark\sql\session.py", line 228, in getOrCreate sc = SparkContext.getOrCreate(sparkConf) File "C:\Spark\spark-3.2.0-bin-hadoop2.7\python\pyspark\context.py", line 392, in getOrCreate SparkContext(conf=conf or SparkConf()) File "C:\Spark\spark-3.2.0-bin-hadoop2.7\python\pyspark\context.py", line 146, in **init** self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer, File "C:\Spark\spark-3.2.0-bin-hadoop2.7\python\pyspark\context.py", line 209, in _do_init self._jsc = jsc or self._initialize_context(self._conf._jconf) File "C:\Spark\spark-3.2.0-bin-hadoop2.7\python\pyspark\context.py", line 329, in _initialize_context return self._jvm.JavaSparkContext(jconf) File "C:\Spark\spark-3.2.0-bin-hadoop2.7\python\lib\py4j-0.10.9.2-src.zip\py4j\java_gateway.py", line 1573, in **call** return_value = get_return_value( File "C:\Spark\spark-3.2.0-bin-hadoop2.7\python\lib\py4j-0.10.9.2-src.zip\py4j\protocol.py", line 326, in get_return_value raise Py4JJavaError( py4j.protocol.Py4JJavaError: An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext. : java.lang.NoClassDefFoundError: Could not initialize class org.apache.spark.storage.StorageUtils$ at org.apache.spark.storage.BlockManagerMasterEndpoint.(BlockManagerMasterEndpoint.scala:110) at org.apache.spark.SparkEnv$.$anonfun$create$9(SparkEnv.scala:348) at org.apache.spark.SparkEnv$.registerOrLookupEndpoint$1(SparkEnv.scala:287) at org.apache.spark.SparkEnv$.create(SparkEnv.scala:336) at org.apache.spark.SparkEnv$.createDriverEnv(SparkEnv.scala:191) at org.apache.spark.SparkContext.createSparkEnv(SparkContext.scala:277) at org.apache.spark.SparkContext.(SparkContext.scala:460) at org.apache.spark.api.java.JavaSparkContext.(JavaSparkContext.scala:58) at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77) at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) at java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:499) at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:480) at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247) at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357) at py4j.Gateway.invoke(Gateway.java:238) at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80) at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69) at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182) at py4j.ClientServerConnection.run(ClientServerConnection.java:106) at java.base/java.lang.Thread.run(Thread.java:833)

(base) C:\WINDOWS\system32>성공: PID 34568인 프로세스(PID 28876인 자식 프로세스)가 종료되었습니다. 성공: PID 28876인 프로세스(PID 39484인 자식 프로세스)가 종료되었습니다. 성공: PID 39484인 프로세스(PID 37972인 자식 프로세스)가 종료되었습니다.
```
</div>
</details>

원인은 pyspark에 맞지 않는 jdk 버전 문제였고 호환되는 jdk는 8,11이라고해 11을 설치 후 설정해주니 문제 없이 실행되었다

![](https://raw.githubusercontent.com/Cloudblack/Forpicture/image//img/20220823174026.png)
