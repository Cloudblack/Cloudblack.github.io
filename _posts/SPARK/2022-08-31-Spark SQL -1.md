---
published: true
layout: single
title: Spark SQL
categories: [SPARK]
tags:
  - Spark SQL
toc: true
toc_sticky: true
date created: 수요일, 8월 31일 2022
date modified: 수요일, 8월 31일 2022
---

# Spark SQL

## Spark sql의 목적
- 스파크 프로그래밍 내부에서 관계형 처리를 하기 위해
- 스키마의 정보를 이용해 자동으로 최적화를 하기 위해
- 외부 데이터셋을 사용하기 쉽게 하기 위해

## Spark sql은?
- 스파크 위에 구현된 하나의 패키지
- 3개의 주요 API
	- SQL
		- Spark Core에 SparkContext가 있다면 Spark SQL에는 SparkSession이있다.
		
		``` python
		from pyspark.sql import SparkSession
		spark = SparkSession.builder.appName("test-app").getOrCreate()
		```
		
	- DataFrame
		- Spark Core에 RDD가있다면 Spark SQl엔 DataFrame이있다.
		- DataFreame은 테이블 데이터셋
		- 개념적으론 RDD에 스키마가 적용된 것같은 느낌
		- DataFrame 만들기
			- RDD에서 스키마를 정의한 다음 변형
			- CSV, JSON등의 데이터를 받는다.
	- Datasets
		- Type이있는 DataFrame
		- Pyspark에선 크게 신경쓰지않아도 된다. (python이 크게 type에 신경을 안쓴다)
- 2개의 백엔드 컴포넌트
	- Catalyst - 쿼리 최적화 엔진
	- Tungsten - 시리얼라이저(용량을 최적화)

## RDD로부터 DataFrame 만들기
- Schema를 자동으로 유추해서 DataFrame만들기
- Schema를 사용자가 정의하기

### RDD
``` python
lines = sc.textFile("example.csv")
data = lines.map(lambda x: x.split(","))
preprocessed = data.map(lambda x: Row(name=x[0]), price = int(x[1]))
```

### Infer (자동으로 만들어줌)
``` python
df = spark.createDataFrame(preprocessed)
```

### Specify (사용자지정)
``` python
schema = StructType(
	StructField("name", StringType(), True),
	StructField("price", StringType(), True)
)
df = spark.createDataFrame(preprocessed,schema).show()
```

## 파일로부터 DataFrame만들기
``` python
from pyspark.sql import SparkSession
spark = SparkSession.builder.appName("test-app").getOrCreate()

#JSON
dataframe = spark.read.json('dataset/nyt2.json')
#TXT
dataframe_txt = spark.read.text('text_data.txt')
#csv 
dataframe_csv = spark.read.csv('csv_data.csv')
#parquet 
dataframe_parquet = spark.read.loda('parquet_data.parquet')
```

## DataFrame을 하나의 데이터베이스 테이블처럼 사용하려면  
- createOrReplaceTempView() 함수로 temporary view를 만들어줘야한다

``` python
data.createOrReplaceTempView("mobility_data")
spark.sql("SELECT pickup_datetime FROM mobility_data LIMIT 5").show()
```

## Spark에서 사용할 수 있는 SQL문
- HIVE Query Language와 거의동일하다
- Select
- From
- Where
- Count
- Having
- Group By
- Order By
- Sort By
- Distinct
- Join
- 등등

## python에서 Spark SQL을 사용하기
- pyspark를 사용한다 => SparkSession
	- SparkSession으로 불러오는 데이터는 DataFrame
	
	``` python
		from pyspark.sql import SparkSession
		spark = SparkSession.builder.appName("test-app").getOrCreate()
	```
	
- SQL문을 사용해서 쿼리가 가능하다
	
	``` python
	data.createOrReplaceTempView("mobility_data")
	spark.sql("SELECT pickup_datetime FROM mobility_data LIMIT 5").show()  
	```
	
- 함수를 사용해서 쿼리도 가능하다

``` python
#select everybody,but increment the age by 1
df.select(df['name'], df['age']+1).show()

#select people older than 21
df.filter(df['age'] >21).show()
```

- DataFrame을 RDD로 변환해 사용할수도있다.

``` python
RDD = df.rdd.map(tuple)
```

- 하지만 RDD를 덜 사용하는 쪽이 좋다 (DataFrame의 이점)
	- MLLIb이나 Spark Streaming같은 다른 스파크 모듈들과 사용하기 편하다
	- 개발하기 편하다
	- 최적화도 알아서 된다.
