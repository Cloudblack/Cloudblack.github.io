---
published: true
layout: post
title: "Airflow란?"
categories:
  - Workflow_scheduling
tags:
  - [Airflow]

toc: true
toc_sticky: true


---

# Airflow란 무엇인가..?

[TOC]

# 배경

데이터 시대에서 데이터 소스와 복잡성과 다양성을 감안할때 서로다른 시스템에 분산된데이터를 연결하려면 데이터 파이프라인이 필요하다

데이터 파이프라인은 분석 보고 기타 비지니스에 활용될 수 있는 데이터를 항상 확보하고 Da나 DS의 시간을 낭비하지않고 효율적으로 사용할 수 있게 해준다

하지만 데이터 파이프라인 구축은 쉽지않으며 점점 더 커지고 복잡해진다

그래서 전체의 프로세스를 오케스트레이션 할 수 있는 Airflow를 사용할 필요가 있다

# Airflow란?

AirFLOW는 복잡한 워크플로우를 프로그래밍 방식으로 작성해서,  스케줄링 및 모니터링과 트러블 슈팅을 편리하게 하는 플랫폼이다

데이트 파이프라인에서 ETl 스크립트들을 스케줄링 할 때 crontab, cloudwatch등을 사용하는 곳이 많은데 스크립트들이 많아지고 서로에 대해 의존성이 생기면서 컨트롤하기 어렵고, 기존 작업이 실패했을 때 다시 스크립트를 실행하려면 로그를 확인하고 실행해야 하는 등의 문제점이 생긴다. 문제점을 바로 복구 하기도 어렵고 어디서 잘못 되었는지, 의존성 있는 스크립트가 잘못되었는데 다음 스크립트가 실행되어버리는 등의 문제점이 발생할 수 있다

그런면에서 Airflow는 서로에 대한 의존성을 표현할 수 있고 스크립트 실패시 알람을 보내 줘 쉽게 수정 및 재시도를 할 수 있고 이전 날짜 작업이 실패했을때 그 날짜만 다시 실행하는 등 파이프라인 시각화, 진행 상황 모니터링 및 문제 해결을 위한 풍부한 사용자 인터페이스 제공한다

워크 플로우는 DAG ( Directed Acyclic Graphs) 형식의 python 코드로 작성 할 수 있어
Python으로 가능한 대부분의 작업들을, Airflow 파이프라인에서 처리가 가능하다

# DAG란?

![img](https://raw.githubusercontent.com/Cloudblack/Forpicture/image/img/img.png)



DAG(Directed Acyclic Graph)는 순환그래프가 아닌 비순환 그래프로 순환하는 싸이클이 존재하지않고 일방향성만 갖는 그래프이며 종속된 task끼리의 순서를 정하는데 좋다

- [이전에 DAG를 활용한 API를 만들때 팀원들에게 설명하려고 쓴 글 링크](https://cloudblack.notion.site/df0a223bd7214805ae1c327277b600a9)

​    

### 그럼 왜 airflow는 DAG를 사용할까?

Task들은 실행 순서가 Sequential함이 보장이 되어야하기 때문이다

가령 라면을 끓이는데

1. 물올리기 2.물끓이기 3.면 넣기 4. 스프넣기
    면 넣기나 스프넣기는 바뀌어도 되지만 물을 끓이기도 전에 면부터 넣는다면 눈물젖은 라면을 먹게될 것이다

마찬 가지로 pipeline 또한 꼭 실행되어야할, 필요한 task가 끝난 후 (api나 db를 동작 등) 다음 task가 동작해야 pipeline이 제대로 작동할 것이다 

만약 Pipelining Task 중에 하나라도 문제가 생기면 어떻게될까?
*API*가 정상적으로 작동하지 않거나, 메모리 부족으로 Spark에서 문제가 발생하거나, *DB*에 작업 내용이 저장이 되지 않는다면?

만약 이러한 데이터 파이프라인을 수십, 수백개를 관리해야한다면?

# Airflow의 구성

<img src="https://raw.githubusercontent.com/Cloudblack/Forpicture/image/img/k8sdataeng1.png" alt="img" style="zoom:50%;" />



 위 그림은 [라인 기술 블로그](https://engineering.linecorp.com/ko/blog/data-engineering-with-airflow-k8s-1/)에서 인용하였습니다.

- **Scheduler** : Workflow를 스케줄링하는 스케줄러 데몬이다. Airflow에서 가장 핵심이 되는 컴포넌트
- Web Server : 앞서 언급한 Airflow의 웹 인터페이스를 제공하는 웹 서버. Flask와 Gunicorn을 이용하여 인터페이스를 제공
- MetaStore(MetaDB) : 메타데이터가 저장되는 데이터베이스. 주로 Postgresql을 추천하지만, [SQL Alchemy](https://ulfrid.github.io/python/python-sqlalchemy/)와 호환 가능한 MySQL이나 SQLite도 이용이 가능하다.
- **Executor** : 어떤 환경에서 Task가 실행(Execute)될지에 대한 타입 정의
    *Debug Executor, Local Executor, Sequential Executor, Celery Executor , etc..* [Executor Type Doc](https://airflow.apache.org/docs/apache-airflow/stable/executor/index.html#)
- **Worker** : 실제 Task를 처리하는 컴포넌트
    Executor는 어떻게 실행될지에 대한 정의를 담당, Worker는 실제 프로세스 작업



###### Operator

- Task의 Wrapper 역할
    원하는 작업을 달성하기 위해 존재한다
    예를 들어, DB에 연결하여 데이터를 Insert 하고 싶다면 Operator를 이용하여야 작업을 할 수 있다.

    [Operator Type](https://airflow.apache.org/docs/apache-airflow/stable/_api/airflow/operators/index.html)에 대해 알아보자.

###### Action Operator

기능이나 명령을 실행하는 Operator.

- Bash Operator
- Python Operator
- etc..

###### Transfer Operator

데이터를 Source에서 Destination으로 전송해주는 Operator.
예를 들어, Presto에서 MySQL로 데이터를 전송하는데에 사용

###### Sensor Operator

특정 조건을 Sensing하여 실행되는 Operator.
다른 Operator들과는 달리 조건이 만족할 때까지 기다렸다가, 조건이 충족되면 다음 Task를 실행하도록 함.
예를 들어, 특정 위치에 파일이 생성되었을 때 다음 Task를 실행하도록 File sensor를 사용할 수 있음

###### Task & Task Instance

Task

- Task는 데이터 파이프라인에 존재하는 Operator를 의미한다

Task Instance

- Task Instance는 데이터 파이프라인이 Trigger되어 실행될 때 생성된 Task를 Task Instance라고 한다.
    OOP에 대한 이해가 있다면, Task는 Class, Task Instance는 Object Instance라고 보면 이해가 쉬울 것 같다.

###### Workflow

워크플로우는 앞서 언급한 모든 개념들을 조합하여 만들어진 개념이다.
DAG를 통해 각 태스크 간의 디펜던시를 정의하고, 각 태스크는 Operator로 실행하는 이러한 일련의 과정들을 통해 워크플로우는 정의된다.

